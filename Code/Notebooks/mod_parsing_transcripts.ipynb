{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa65ca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb69210",
   "metadata": {},
   "source": [
    "## Importing Selenium and Drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8f35ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d50fb3",
   "metadata": {},
   "source": [
    "## Importing Data Processing Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9e6f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622c7f4a",
   "metadata": {},
   "source": [
    "## Import User-Defined Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43a377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycodes import mod_utility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130881ff",
   "metadata": {},
   "source": [
    "## Creating User-Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18464fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ticker(transcript_header:str) -> str:\n",
    "    \"\"\"\n",
    "    \n",
    "    Parses out the ticker symbol from the transcript's header \n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    transcript_header (str) : header of the transcripts \n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    ticker_cd (str) : Ticker Symbol Code \n",
    "    \n",
    "    \"\"\"    \n",
    "    try:\n",
    "        ticker_cd = transcript_header[transcript_header.rfind(r'(')+1 : transcript_header.rfind(r')')]\n",
    "        return ticker_cd\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d398f236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(company_name:str):\n",
    "    \"\"\"\n",
    "    \n",
    "    Get ticker symbol from Company Name\n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    company_name (str) : Company Name\n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    company_code (str) : Company code from Yahoo Finance  \n",
    "    \n",
    "    \"\"\"    \n",
    "    \n",
    "    yfinance = \"https://query2.finance.yahoo.com/v1/finance/search\"\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "    params = {\"q\": company_name, \"quotes_count\": 1, \"country\": \"United States\"}\n",
    "\n",
    "    \n",
    "    res = requests.get(url=yfinance, params=params, headers={'User-Agent': user_agent})\n",
    "    data = res.json()\n",
    "\n",
    "    try:\n",
    "        meta_data = data['quotes'][0]\n",
    "        exchange = meta_data['exchange']\n",
    "        sector = meta_data['sector']\n",
    "        industry = meta_data['industry']\n",
    "    \n",
    "    except:\n",
    "        exchange = ''\n",
    "        sector = ''\n",
    "        industry = ''\n",
    "    \n",
    "    return [ exchange , sector , industry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c28a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls(driver_data,params_bs4_filter : dict) -> list:\n",
    "    \"\"\"\n",
    "    \n",
    "    Extract embedded urls from the main page in form of [header,link] pairs\n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    driver_data (Selenium.webdriver.Chrome object) : Contains data about the page that needs to be parsed \n",
    "    params_bs4_filter (dict) : Dictionary of parameters that are required for effective parsing using BeatifulSoup\n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    parsed_links_list (list) : List of [header,link] for embedded urls in the main page\n",
    "    \n",
    "    \"\"\"      \n",
    "    \n",
    "    page_content_str = None\n",
    "    bs4_soup_data_list = None\n",
    "    parsed_links_list = []\n",
    "    \n",
    "    page_content_str = driver_data.page_source\n",
    "    bs4_soup_data_list = BeautifulSoup(page_content_str)\n",
    "    \n",
    "    for links in bs4_soup_data_list.findAll(params_bs4_filter['name'],\n",
    "                                            href=params_bs4_filter['href'], \n",
    "                                            attrs=params_bs4_filter['attrs'],\n",
    "                                            recursive=params_bs4_filter['recursive']):\n",
    "        link = links['href']\n",
    "        header = links.contents[0]\n",
    "        \n",
    "        parsed_links_list.append([header,link])\n",
    "    \n",
    "    return parsed_links_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc87d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page_data_for_url(driver ,params_bs4_filter : dict):\n",
    "    \"\"\"\n",
    "    \n",
    "    Extract embedded urls from the main page in form of Pandas Dataframe\n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    url (str) : url from which data is to be parsed \n",
    "    params_iter (dict) : Dictionary of parameters that are required to iterate over the web page\n",
    "    params_bs4_filter (dict) : Dictionary of parameters that are required for effective parsing using BeatifulSoup\n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    extracted_url_df (Pandas Dataframe) : Pandas Dataframe with header and corresponding urls\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    ## Get [header,links] pairs for embedded urls \n",
    "    new_results = extract_urls(driver,params_bs4_filter)\n",
    "    \n",
    "    \n",
    "    ## Convert [header,links] pairs to pandas dataframe\n",
    "    extracted_url_df = pd.DataFrame(new_results,columns=['header','link'])\n",
    "    \n",
    "    extracted_url_df[['Org Name','temp']] = extracted_url_df['header'].str.split('(', 1,expand=True)\n",
    "    \n",
    "    extracted_url_df['ticker_cd'] = extracted_url_df['header'].map(get_ticker).to_list()\n",
    "    \n",
    "    extracted_url_df[['stock_exchange','sector','industry']] = extracted_url_df['Org Name'].map(get_metadata).to_list()\n",
    "    \n",
    "    extracted_url_df[['nse_mes','nse_sector','nse_industry','nse_basic_industry']] = extracted_url_df['ticker_cd'].map(get_industry_info).to_list()\n",
    "        \n",
    "    \n",
    "    ## Data cleaning:\n",
    "    ## 1. Remove unwanted rows\n",
    "    extracted_url_df = extracted_url_df[~extracted_url_df['header'].str.contains(\"\\[\",na=True)]\n",
    "    extracted_url_df.reset_index(inplace = True)\n",
    "    extracted_url_df.drop(['index','temp'],axis=1,inplace=True)\n",
    "    \n",
    "    ## 2. Remove duplicate rows\n",
    "    extracted_url_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return extracted_url_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527b5090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity_participants(driver_data , transcript_header : str):\n",
    "    \"\"\"\n",
    "    \n",
    "    Extract participants from the transcripts\n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    driver_data (Selenium.webdriver.Chrome object) : Contains data about the page that needs to be parsed \n",
    "    transcript_header (str) : Company name to which the transcripts belongs to\n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    parsed_links_list (list) : List of ppts data [[],...]\n",
    "    \n",
    "    \"\"\"     \n",
    "    parsed_corp_ppts_list = []\n",
    "    parsed_ppts_list = []\n",
    "    page_content_str = None\n",
    "    bs4_soup_data_list = None\n",
    "    parsed_links_list = []\n",
    "    \n",
    "    page_content_str = driver_data.page_source\n",
    "    bs4_soup_data_list = BeautifulSoup(page_content_str)\n",
    "    \n",
    "    \n",
    "    ## For Corporate PPTs\n",
    "    \n",
    "    params_corp_ppts = {\n",
    "        'tag_val' : 'h2',\n",
    "        'text_val' : 'Corporate Participants:'\n",
    "    }\n",
    "    \n",
    "    target = bs4_soup_data_list.find(params_corp_ppts['tag_val'],\n",
    "                                     text=params_corp_ppts['text_val'])\n",
    "\n",
    "    for sib in target.find_next_siblings():\n",
    "        if sib.name==params_corp_ppts['tag_val']:\n",
    "            break\n",
    "        else:\n",
    "            ppt_corp = transcript_header.split('(', 1)[0]\n",
    "            try:\n",
    "                ppt_name , ppt_desig = sib.text.split(\"\\xa0—\\xa0\")\n",
    "            except:\n",
    "                ppt_name , ppt_desig = sib.text.split(\"\\xa0\")\n",
    "\n",
    "            parsed_corp_ppts_list.append([transcript_header , ppt_name , ppt_desig , ppt_corp])\n",
    "    \n",
    "          \n",
    "    ## For Analysts\n",
    "    \n",
    "    params_analyst_ppts = {\n",
    "        'tag_val' : 'h2',\n",
    "        'text_val' : 'Analysts:'\n",
    "    }\n",
    "    \n",
    "    target = bs4_soup_data_list.find(params_analyst_ppts['tag_val'],\n",
    "                                     text=params_analyst_ppts['text_val'])\n",
    "    \n",
    "\n",
    "    for sib in target.find_next_siblings():\n",
    "        if sib.name==params_analyst_ppts['tag_val']:\n",
    "            break\n",
    "        else:\n",
    "            ppt_name , _ , ppt_corp_x_desig = sib.text.split(\"\\xa0\")\n",
    "            \n",
    "            ppt_corp , ppt_desig =  ppt_corp_x_desig.split(\"—\")\n",
    "\n",
    "            parsed_ppts_list.append([transcript_header , ppt_name , ppt_desig , ppt_corp])\n",
    "            \n",
    "    \n",
    "    \n",
    "    return parsed_corp_ppts_list,parsed_ppts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811eaa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcripts_urls_from_url(url:str,params_iter : dict,params_bs4_filter : dict):\n",
    "    \"\"\"\n",
    "    \n",
    "    Extract url of transcripts from main url\n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    url (str) : url from which data is to be parsed \n",
    "    params_iter (dict) : Dictionary of parameters that are required to iterate over the web page\n",
    "    params_bs4_filter (dict) : Dictionary of parameters that are required for effective parsing using BeatifulSoup\n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    extracted_url_df (pandas dataframe) : Dataframe of extrated urls\n",
    "    \n",
    "    \"\"\"   \n",
    "    \n",
    "    sel_driver = get_driver_data(url , params_iter)\n",
    "    \n",
    "    extracted_url_df = parse_page_data_for_url(sel_driver , params_bs4_filter)\n",
    "    \n",
    "    return extracted_url_df , sel_driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1956da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_disclosures_from_transcripts(driver_data,pading_cols:dict):\n",
    "    \n",
    "    page_content_str = driver_data.page_source\n",
    "    bs4_soup_data_list = BeautifulSoup(page_content_str)\n",
    "    \n",
    "    header = None\n",
    "\n",
    "    value_list = []\n",
    "\n",
    "    for values in bs4_soup_data_list.find('h2',text='Presentation:').find_next_siblings():\n",
    "\n",
    "        if values.find('span') is not None:\n",
    "            continue\n",
    "\n",
    "        if values.find('strong') is not None:\n",
    "            header = values.text\n",
    "\n",
    "        else:\n",
    "            value_list.append([header,values.text])\n",
    "\n",
    "    data_df = pd.DataFrame(value_list,columns=['said_by','info'])\n",
    "    \n",
    "    data_df[list(pading_cols.keys())] = list(pading_cols.values())\n",
    "    \n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d897549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_answers_from_transcripts(driver_data,pading_cols:dict):\n",
    "    \n",
    "    page_content_str = driver_data.page_source\n",
    "    bs4_soup_data_list = BeautifulSoup(page_content_str)    \n",
    "\n",
    "    header = None\n",
    "\n",
    "    question = []\n",
    "\n",
    "    value_list = []\n",
    "\n",
    "    for values in bs4_soup_data_list.find('h2',text='Questions and Answers:').find_next_siblings():\n",
    "\n",
    "        if values.find('span') is not None:\n",
    "            continue\n",
    "\n",
    "        if values.find('strong') is not None:\n",
    "            header = values.text\n",
    "\n",
    "        else:\n",
    "            if header == 'Operator':\n",
    "                continue\n",
    "            if 'Analyst' in header:\n",
    "                question = [header,values.text]\n",
    "            else:\n",
    "                value_list.append([question[0],question[1],header,values.text])\n",
    "\n",
    "    data_df = pd.DataFrame(value_list,columns=['question_by','question','answer_by','answer'])\n",
    "    \n",
    "    data_df[list(pading_cols.keys())] = list(pading_cols.values())\n",
    "    \n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f885970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_transcripts(extracted_url_df : pd.DataFrame , sel_driver):\n",
    "    \"\"\"\n",
    "    \n",
    "    Extract data from transcripts from url dataframe\n",
    "    1. Participants Data\n",
    "        a. Corporate Participants\n",
    "        b. Analysts Participants\n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    url (str) : url from which data is to be parsed \n",
    "    params_iter (dict) : Dictionary of parameters that are required to iterate over the web page\n",
    "    params_bs4_filter (dict) : Dictionary of parameters that are required for effective parsing using BeatifulSoup\n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    corp_ppts_df (pandas dataframe) : Dataframe of Corporate Participants\n",
    "    analyst_ppts_df (pandas dataframe) : Dataframe of Analysts Participants\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    corp_ppts_df = pd.DataFrame()\n",
    "    analyst_ppts_df = pd.DataFrame()\n",
    "    disclosures_df = pd.DataFrame()\n",
    "    qa_df = pd.DataFrame()\n",
    "    \n",
    "    for index,row in extracted_url_df.iterrows():\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            print('Information Retrieval Started -> ',row['Org Name'],'(',row['ticker_cd'],')')\n",
    "            sel_driver.get(row['link'])\n",
    "            \n",
    "            pading_cols = {\n",
    "                \"ticker_cd\" : row['ticker_cd'],\n",
    "                \"Org Name\" : row['Org Name']\n",
    "            }\n",
    "            \n",
    "            print('\\n')\n",
    "            \n",
    "            print('\\tStep 1 : Get Disclosures data','\\n')\n",
    "            inter_disclosures_df = get_disclosures_from_transcripts(sel_driver,pading_cols)\n",
    "            \n",
    "            \n",
    "            print('\\tStep 2 : Get Q/A data','\\n')\n",
    "            inter_qa_df = get_question_answers_from_transcripts(sel_driver,pading_cols)\n",
    "        \n",
    "        \n",
    "            print('\\tStep 3 : Get participants data','\\n')\n",
    "            corp_ppts_list , analyst_ppts_list = extract_entity_participants(sel_driver,row['header'])\n",
    "            \n",
    "            inter_corp_ppts_df = pd.DataFrame(corp_ppts_list,columns=['Transcript Header','Name','Designation','Corp Name'])\n",
    "\n",
    "            inter_analyst_ppts_df = pd.DataFrame(analyst_ppts_list,columns=['Transcript Header','Name','Designation','Corp Name'])\n",
    "            \n",
    "            \n",
    "            \n",
    "            print('\\tStep 4 : Combine Data','\\n')\n",
    "            if corp_ppts_df.shape[1] > 0:\n",
    "                corp_ppts_df = pd.concat(corp_ppts_df,inter_corp_ppts_df)\n",
    "                analyst_ppts_df = pd.concat(analyst_ppts_df,inter_analyst_ppts_df)\n",
    "                disclosures_df = pd.concat(disclosures_df,inter_disclosures_df)\n",
    "                qa_df = pd.concat(qa_df,inter_qa_df)\n",
    "            else:\n",
    "                corp_ppts_df = inter_corp_ppts_df\n",
    "                analyst_ppts_df = inter_analyst_ppts_df\n",
    "                disclosures_df = inter_disclosures_df\n",
    "                qa_df = inter_qa_df\n",
    "                \n",
    "        except:\n",
    "            print('\\r','Information Retrieval Failed -> ',row['Org Name'],'(',row['ticker_cd'],')\\n')\n",
    "            continue\n",
    "        \n",
    "        if index > 2:\n",
    "            return corp_ppts_df , analyst_ppts_df , disclosures_df , qa_df\n",
    "        \n",
    "    return corp_ppts_df , analyst_ppts_df , disclosures_df , qa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d766055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcripts_data_wrapper(url:str,params_iter : dict,params_bs4_filter : dict):\n",
    "    \"\"\"\n",
    "    \n",
    "    Extract data from transcripts from url \n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    url (str) : url from which data is to be parsed \n",
    "    params_iter (dict) : Dictionary of parameters that are required to iterate over the web page\n",
    "    params_bs4_filter (dict) : Dictionary of parameters that are required for effective parsing using BeatifulSoup\n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    corp_ppts_df (pandas dataframe) : Dataframe of Corporate Participants\n",
    "    analyst_ppts_df (pandas dataframe) : Dataframe of Analysts Participants\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    extracted_url_df , sel_driver = get_transcripts_urls_from_url(url , params_iter , params_bs4_filter)\n",
    "    \n",
    "    corp_ppts_df , analyst_ppts_df , disclosures_df , qa_df = get_data_from_transcripts(extracted_url_df , sel_driver)\n",
    "        \n",
    "    return extracted_url_df , corp_ppts_df , analyst_ppts_df , disclosures_df , qa_df\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
