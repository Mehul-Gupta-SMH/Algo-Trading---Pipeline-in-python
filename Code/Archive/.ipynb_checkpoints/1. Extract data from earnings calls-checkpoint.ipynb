{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d949987",
   "metadata": {},
   "source": [
    "# 1. Extract data from earnings calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8d14fa",
   "metadata": {},
   "source": [
    "## Adding required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7168433c",
   "metadata": {},
   "source": [
    "1. *Selenium* : https://www.selenium.dev/\n",
    "2. *Beautiful Soup* : https://beautiful-soup-4.readthedocs.io/en/latest/\n",
    "3. *WebDriver Manager* : https://www.selenium.dev/documentation/webdriver/getting_started/install_drivers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abbbee8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install selenium\n",
    "# !pip install beautifulsoup4\n",
    "# !pip3 install webdriver-manager\n",
    "# !pip3 install yfinance\n",
    "# !pip3 install nsepython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "936fa4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2511b4",
   "metadata": {},
   "source": [
    "## Importing Selenium and Drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25ef9030",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a99097ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd626bd",
   "metadata": {},
   "source": [
    "## Importing Data Processing Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f13f73aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9c7fe4",
   "metadata": {},
   "source": [
    "## 0. Common Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4c3a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_driver_data(url:str,params_iter : dict):\n",
    "    \"\"\"\n",
    "    \n",
    "    Get the html data for the page opened by Selenium driver\n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    url (str) : url from which data is to be parsed \n",
    "    params_iter (dict) : Dictionary of parameters that are required to iterate over the web page\n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    Selenium.webdriver.Chrome object : Contains data about the page that needs to be parsed \n",
    "    \n",
    "    \"\"\"    \n",
    "    \n",
    "    SCROLL_PAUSE_TIME = params_iter['scroll_wait_time']\n",
    "    \n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()),options=options)\n",
    "    \n",
    "#     driver.minimize_window()\n",
    "    \n",
    "    ## Check if the URL has correct format\n",
    "    if not url_validator(url):\n",
    "        raise ValueError(\"The URL \"&url&\" is not a valid URL format\")\n",
    "        pass\n",
    "    \n",
    "    driver.get(url)\n",
    "    \n",
    "    ## Check what was the last height of the page\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    ## Get the whole page data by loading all data from lazy loading page\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "        \n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        if (new_height == last_height) or (iteration == params_iter['iter_threshold']):\n",
    "            break\n",
    "            \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce509b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_validator(url:str) -> bool:\n",
    "    \"\"\"\n",
    "    \n",
    "    Validates if the url have correct format\n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    url (str) : url string to be checked \n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    Bool \n",
    "    \n",
    "    \"\"\"\n",
    "    regex = re.compile(\n",
    "            r'^(?:http|ftp)s?://' # http:// or https://\n",
    "            r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|' #domain...\n",
    "            r'localhost|' #localhost...\n",
    "            r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})' # ...or ip\n",
    "            r'(?::\\d+)?' # optional port\n",
    "            r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\n",
    "    \n",
    "    if re.match(regex, url):\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd8f78ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_industry_info(ticker,trial=0):\n",
    "    \n",
    "    if trial > 3:\n",
    "        print('Failed for ticker => ',ticker)\n",
    "        return ['','','','']\n",
    "    \n",
    "    if (ticker is None) or (ticker.strip() == ''):\n",
    "        return ['','','','']\n",
    "    \n",
    "    industry_data_df = pd.read_csv('industry.csv')\n",
    "    \n",
    "    filtered_industry_data_df = industry_data_df[industry_data_df['ticker']==ticker]\n",
    "    \n",
    "    if filtered_industry_data_df.shape[0]>0:\n",
    "        return [\n",
    "            filtered_industry_data_df['Macro-Economic Sector'].iat[0],\n",
    "            filtered_industry_data_df['Sector'].iat[0],\n",
    "            filtered_industry_data_df['Industry'].iat[0],\n",
    "            filtered_industry_data_df['Basic Industry'].iat[0]\n",
    "        ]\n",
    "    \n",
    "    else:\n",
    "        print(ticker)\n",
    "        \n",
    "        url = 'https://www.nseindia.com/get-quotes/equity?symbol={}'.format(ticker)\n",
    "\n",
    "        params_iter = {}\n",
    "        params_iter['scroll_wait_time'] = 5.0\n",
    "        params_iter['iter_threshold'] = 1\n",
    "\n",
    "        driver_data = get_driver_data(url , params_iter)\n",
    "\n",
    "        page_content_str = driver_data.page_source\n",
    "        bs4_soup_data_list = BeautifulSoup(page_content_str)\n",
    "\n",
    "        driver_data.close()\n",
    "\n",
    "        for table in bs4_soup_data_list.findAll('table'):\n",
    "            if table.attrs.get('id','') == 'industryInfo':\n",
    "                header = list(table.stripped_strings)[:4]\n",
    "                body = list(table.stripped_strings)[4:]\n",
    "                \n",
    "                try:\n",
    "                    industry_data_df.loc[len(industry_data_df)] = [ticker,'NSE',body[0],body[1],body[2],body[3]] \n",
    "\n",
    "                    industry_data_df.to_csv('industry.csv',index=False)\n",
    "                except:\n",
    "                    trial += 1\n",
    "                    return get_industry_info(ticker,trial)\n",
    "                \n",
    "                return body\n",
    "            \n",
    "        return ['','','','']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c63c813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_historical_data(symbol,start_date,end_date):\n",
    "    \n",
    "    payload_df = pd.DataFrame()\n",
    "    baseurl = \"https://www.nseindia.com/\"\n",
    "    series = \"EQ\"\n",
    "    headers = {\n",
    "        'Connection': 'keep-alive',\n",
    "        'Cache-Control': 'max-age=0',\n",
    "        'DNT': '1',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.79 Safari/537.36',\n",
    "        'Sec-Fetch-User': '?1',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
    "        'Sec-Fetch-Site': 'none',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Accept-Language': 'en-US,en;q=0.9,hi;q=0.8',\n",
    "    }\n",
    "    \n",
    "    \n",
    "    dt_start_date = datetime.datetime.strptime(start_date,\"%d-%m-%Y\")\n",
    "    dt_end_date = datetime.datetime.strptime(end_date,\"%d-%m-%Y\")\n",
    "    \n",
    "    dt_inter_end_date = dt_start_date\n",
    "    \n",
    "    while (dt_end_date - dt_inter_end_date).days > 0:\n",
    "        \n",
    "        print(\"\\r\",dt_start_date,\" to \" , dt_inter_end_date , \" with days \" ,abs((dt_inter_end_date - dt_start_date).days))\n",
    "        \n",
    "        dt_inter_end_date = dt_inter_end_date + datetime.timedelta(days=40)\n",
    "        dt_inter_end_date = min([dt_inter_end_date,dt_end_date])\n",
    "        \n",
    "        inter_end_date = datetime.datetime.strftime(dt_inter_end_date,\"%d-%m-%Y\")\n",
    "        \n",
    "        url=\"https://www.nseindia.com/api/historical/cm/equity?symbol=\"+symbol+\"&series=[%22\"+series+\"%22]&from=\"+str(start_date)+\"&to=\"+str(inter_end_date)+\"\"\n",
    "    \n",
    "        session = requests.Session()\n",
    "        request = session.get(baseurl, headers=headers, timeout=5)\n",
    "        cookies = dict(request.cookies)\n",
    "        payload = session.get(url, headers=headers, timeout=5, cookies=cookies).json()\n",
    "\n",
    "        inter_payload_df = pd.DataFrame(payload['data'])\n",
    "        \n",
    "        if payload_df.shape[0]:\n",
    "            payload_df = pd.concat([payload_df,inter_payload_df])\n",
    "        else:\n",
    "            payload_df = inter_payload_df\n",
    "        \n",
    "    return payload_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01fa37c",
   "metadata": {},
   "source": [
    "## 1. Parsing Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2edcab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ticker(transcript_header:str) -> str:\n",
    "    \"\"\"\n",
    "    \n",
    "    Parses out the ticker symbol from the transcript's header \n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    transcript_header (str) : header of the transcripts \n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    ticker_cd (str) : Ticker Symbol Code \n",
    "    \n",
    "    \"\"\"    \n",
    "    try:\n",
    "        ticker_cd = transcript_header[transcript_header.rfind(r'(')+1 : transcript_header.rfind(r')')]\n",
    "        return ticker_cd\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8351e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(company_name:str):\n",
    "    \"\"\"\n",
    "    \n",
    "    Get ticker symbol from Company Name\n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    company_name (str) : Company Name\n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    company_code (str) : Company code from Yahoo Finance  \n",
    "    \n",
    "    \"\"\"    \n",
    "    \n",
    "    yfinance = \"https://query2.finance.yahoo.com/v1/finance/search\"\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "    params = {\"q\": company_name, \"quotes_count\": 1, \"country\": \"United States\"}\n",
    "\n",
    "    \n",
    "    res = requests.get(url=yfinance, params=params, headers={'User-Agent': user_agent})\n",
    "    data = res.json()\n",
    "\n",
    "    try:\n",
    "        meta_data = data['quotes'][0]\n",
    "        exchange = meta_data['exchange']\n",
    "        sector = meta_data['sector']\n",
    "        industry = meta_data['industry']\n",
    "    \n",
    "    except:\n",
    "        exchange = ''\n",
    "        sector = ''\n",
    "        industry = ''\n",
    "    \n",
    "    return [ exchange , sector , industry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aae27ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls(driver_data,params_bs4_filter : dict) -> list:\n",
    "    \"\"\"\n",
    "    \n",
    "    Extract embedded urls from the main page in form of [header,link] pairs\n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    driver_data (Selenium.webdriver.Chrome object) : Contains data about the page that needs to be parsed \n",
    "    params_bs4_filter (dict) : Dictionary of parameters that are required for effective parsing using BeatifulSoup\n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    parsed_links_list (list) : List of [header,link] for embedded urls in the main page\n",
    "    \n",
    "    \"\"\"      \n",
    "    \n",
    "    page_content_str = None\n",
    "    bs4_soup_data_list = None\n",
    "    parsed_links_list = []\n",
    "    \n",
    "    page_content_str = driver_data.page_source\n",
    "    bs4_soup_data_list = BeautifulSoup(page_content_str)\n",
    "    \n",
    "    for links in bs4_soup_data_list.findAll(params_bs4_filter['name'],\n",
    "                                            href=params_bs4_filter['href'], \n",
    "                                            attrs=params_bs4_filter['attrs'],\n",
    "                                            recursive=params_bs4_filter['recursive']):\n",
    "        link = links['href']\n",
    "        header = links.contents[0]\n",
    "        \n",
    "        parsed_links_list.append([header,link])\n",
    "    \n",
    "    return parsed_links_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf962e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page_data_for_url(driver ,params_bs4_filter : dict):\n",
    "    \"\"\"\n",
    "    \n",
    "    Extract embedded urls from the main page in form of Pandas Dataframe\n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    url (str) : url from which data is to be parsed \n",
    "    params_iter (dict) : Dictionary of parameters that are required to iterate over the web page\n",
    "    params_bs4_filter (dict) : Dictionary of parameters that are required for effective parsing using BeatifulSoup\n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    extracted_url_df (Pandas Dataframe) : Pandas Dataframe with header and corresponding urls\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    ## Get [header,links] pairs for embedded urls \n",
    "    new_results = extract_urls(driver,params_bs4_filter)\n",
    "    \n",
    "    \n",
    "    ## Convert [header,links] pairs to pandas dataframe\n",
    "    extracted_url_df = pd.DataFrame(new_results,columns=['header','link'])\n",
    "    \n",
    "    extracted_url_df[['Org Name','temp']] = extracted_url_df['header'].str.split('(', 1,expand=True)\n",
    "    \n",
    "    extracted_url_df['ticker_cd'] = extracted_url_df['header'].map(get_ticker).to_list()\n",
    "    \n",
    "    extracted_url_df[['stock_exchange','sector','industry']] = extracted_url_df['Org Name'].map(get_metadata).to_list()\n",
    "    \n",
    "    extracted_url_df[['nse_mes','nse_sector','nse_industry','nse_basic_industry']] = extracted_url_df['ticker_cd'].map(get_industry_info).to_list()\n",
    "        \n",
    "    \n",
    "    ## Data cleaning:\n",
    "    ## 1. Remove unwanted rows\n",
    "    extracted_url_df = extracted_url_df[~extracted_url_df['header'].str.contains(\"\\[\",na=True)]\n",
    "    extracted_url_df.reset_index(inplace = True)\n",
    "    extracted_url_df.drop(['index','temp'],axis=1,inplace=True)\n",
    "    \n",
    "    ## 2. Remove duplicate rows\n",
    "    extracted_url_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return extracted_url_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c14f285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity_participants(driver_data , transcript_header : str):\n",
    "    \"\"\"\n",
    "    \n",
    "    Extract participants from the transcripts\n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    driver_data (Selenium.webdriver.Chrome object) : Contains data about the page that needs to be parsed \n",
    "    transcript_header (str) : Company name to which the transcripts belongs to\n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    parsed_links_list (list) : List of ppts data [[],...]\n",
    "    \n",
    "    \"\"\"     \n",
    "    parsed_corp_ppts_list = []\n",
    "    parsed_ppts_list = []\n",
    "    page_content_str = None\n",
    "    bs4_soup_data_list = None\n",
    "    parsed_links_list = []\n",
    "    \n",
    "    page_content_str = driver_data.page_source\n",
    "    bs4_soup_data_list = BeautifulSoup(page_content_str)\n",
    "    \n",
    "    \n",
    "    ## For Corporate PPTs\n",
    "    \n",
    "    params_corp_ppts = {\n",
    "        'tag_val' : 'h2',\n",
    "        'text_val' : 'Corporate Participants:'\n",
    "    }\n",
    "    \n",
    "    target = bs4_soup_data_list.find(params_corp_ppts['tag_val'],\n",
    "                                     text=params_corp_ppts['text_val'])\n",
    "\n",
    "    for sib in target.find_next_siblings():\n",
    "        if sib.name==params_corp_ppts['tag_val']:\n",
    "            break\n",
    "        else:\n",
    "            ppt_corp = transcript_header.split('(', 1)[0]\n",
    "            try:\n",
    "                ppt_name , ppt_desig = sib.text.split(\"\\xa0窶能\xa0\")\n",
    "            except:\n",
    "                ppt_name , ppt_desig = sib.text.split(\"\\xa0\")\n",
    "\n",
    "            parsed_corp_ppts_list.append([transcript_header , ppt_name , ppt_desig , ppt_corp])\n",
    "    \n",
    "          \n",
    "    ## For Analysts\n",
    "    \n",
    "    params_analyst_ppts = {\n",
    "        'tag_val' : 'h2',\n",
    "        'text_val' : 'Analysts:'\n",
    "    }\n",
    "    \n",
    "    target = bs4_soup_data_list.find(params_analyst_ppts['tag_val'],\n",
    "                                     text=params_analyst_ppts['text_val'])\n",
    "    \n",
    "\n",
    "    for sib in target.find_next_siblings():\n",
    "        if sib.name==params_analyst_ppts['tag_val']:\n",
    "            break\n",
    "        else:\n",
    "            ppt_name , _ , ppt_corp_x_desig = sib.text.split(\"\\xa0\")\n",
    "            \n",
    "            ppt_corp , ppt_desig =  ppt_corp_x_desig.split(\"窶能")\n",
    "\n",
    "            parsed_ppts_list.append([transcript_header , ppt_name , ppt_desig , ppt_corp])\n",
    "            \n",
    "    \n",
    "    \n",
    "    return parsed_corp_ppts_list,parsed_ppts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a744861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcripts_urls_from_url(url:str,params_iter : dict,params_bs4_filter : dict):\n",
    "    \"\"\"\n",
    "    \n",
    "    Extract url of transcripts from main url\n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    url (str) : url from which data is to be parsed \n",
    "    params_iter (dict) : Dictionary of parameters that are required to iterate over the web page\n",
    "    params_bs4_filter (dict) : Dictionary of parameters that are required for effective parsing using BeatifulSoup\n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    extracted_url_df (pandas dataframe) : Dataframe of extrated urls\n",
    "    \n",
    "    \"\"\"   \n",
    "    \n",
    "    sel_driver = get_driver_data(url , params_iter)\n",
    "    \n",
    "    extracted_url_df = parse_page_data_for_url(sel_driver , params_bs4_filter)\n",
    "    \n",
    "    return extracted_url_df , sel_driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97b9bcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_disclosures_from_transcripts(driver_data,pading_cols:dict):\n",
    "    \n",
    "    page_content_str = driver_data.page_source\n",
    "    bs4_soup_data_list = BeautifulSoup(page_content_str)\n",
    "    \n",
    "    header = None\n",
    "\n",
    "    value_list = []\n",
    "\n",
    "    for values in bs4_soup_data_list.find('h2',text='Presentation:').find_next_siblings():\n",
    "\n",
    "        if values.find('span') is not None:\n",
    "            continue\n",
    "\n",
    "        if values.find('strong') is not None:\n",
    "            header = values.text\n",
    "\n",
    "        else:\n",
    "            value_list.append([header,values.text])\n",
    "\n",
    "    data_df = pd.DataFrame(value_list,columns=['said_by','info'])\n",
    "    \n",
    "    data_df[list(pading_cols.keys())] = list(pading_cols.values())\n",
    "    \n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e148646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_answers_from_transcripts(driver_data,pading_cols:dict):\n",
    "    \n",
    "    page_content_str = driver_data.page_source\n",
    "    bs4_soup_data_list = BeautifulSoup(page_content_str)    \n",
    "\n",
    "    header = None\n",
    "\n",
    "    question = []\n",
    "\n",
    "    value_list = []\n",
    "\n",
    "    for values in bs4_soup_data_list.find('h2',text='Questions and Answers:').find_next_siblings():\n",
    "\n",
    "        if values.find('span') is not None:\n",
    "            continue\n",
    "\n",
    "        if values.find('strong') is not None:\n",
    "            header = values.text\n",
    "\n",
    "        else:\n",
    "            if header == 'Operator':\n",
    "                continue\n",
    "            if 'Analyst' in header:\n",
    "                question = [header,values.text]\n",
    "            else:\n",
    "                value_list.append([question[0],question[1],header,values.text])\n",
    "\n",
    "    data_df = pd.DataFrame(value_list,columns=['question_by','question','answer_by','answer'])\n",
    "    \n",
    "    data_df[list(pading_cols.keys())] = list(pading_cols.values())\n",
    "    \n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a12684ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_transcripts(extracted_url_df : pd.DataFrame , sel_driver):\n",
    "    \"\"\"\n",
    "    \n",
    "    Extract data from transcripts from url dataframe\n",
    "    1. Participants Data\n",
    "        a. Corporate Participants\n",
    "        b. Analysts Participants\n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    url (str) : url from which data is to be parsed \n",
    "    params_iter (dict) : Dictionary of parameters that are required to iterate over the web page\n",
    "    params_bs4_filter (dict) : Dictionary of parameters that are required for effective parsing using BeatifulSoup\n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    corp_ppts_df (pandas dataframe) : Dataframe of Corporate Participants\n",
    "    analyst_ppts_df (pandas dataframe) : Dataframe of Analysts Participants\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    corp_ppts_df = pd.DataFrame()\n",
    "    analyst_ppts_df = pd.DataFrame()\n",
    "    disclosures_df = pd.DataFrame()\n",
    "    qa_df = pd.DataFrame()\n",
    "    \n",
    "    for index,row in extracted_url_df.iterrows():\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            print('Information Retrieval Started -> ',row['Org Name'],'(',row['ticker_cd'],')')\n",
    "            sel_driver.get(row['link'])\n",
    "            \n",
    "            pading_cols = {\n",
    "                \"ticker_cd\" : row['ticker_cd'],\n",
    "                \"Org Name\" : row['Org Name']\n",
    "            }\n",
    "            \n",
    "            print('\\n')\n",
    "            \n",
    "            print('\\tStep 1 : Get Disclosures data','\\n')\n",
    "            inter_disclosures_df = get_disclosures_from_transcripts(sel_driver,pading_cols)\n",
    "            \n",
    "            \n",
    "            print('\\tStep 2 : Get Q/A data','\\n')\n",
    "            inter_qa_df = get_question_answers_from_transcripts(sel_driver,pading_cols)\n",
    "        \n",
    "        \n",
    "            print('\\tStep 3 : Get participants data','\\n')\n",
    "            corp_ppts_list , analyst_ppts_list = extract_entity_participants(sel_driver,row['header'])\n",
    "            \n",
    "            inter_corp_ppts_df = pd.DataFrame(corp_ppts_list,columns=['Transcript Header','Name','Designation','Corp Name'])\n",
    "\n",
    "            inter_analyst_ppts_df = pd.DataFrame(analyst_ppts_list,columns=['Transcript Header','Name','Designation','Corp Name'])\n",
    "            \n",
    "            \n",
    "            \n",
    "            print('\\tStep 4 : Combine Data','\\n')\n",
    "            if corp_ppts_df.shape[1] > 0:\n",
    "                corp_ppts_df = pd.concat(corp_ppts_df,inter_corp_ppts_df)\n",
    "                analyst_ppts_df = pd.concat(analyst_ppts_df,inter_analyst_ppts_df)\n",
    "                disclosures_df = pd.concat(disclosures_df,inter_disclosures_df)\n",
    "                qa_df = pd.concat(qa_df,inter_qa_df)\n",
    "            else:\n",
    "                corp_ppts_df = inter_corp_ppts_df\n",
    "                analyst_ppts_df = inter_analyst_ppts_df\n",
    "                disclosures_df = inter_disclosures_df\n",
    "                qa_df = inter_qa_df\n",
    "                \n",
    "        except:\n",
    "            print('\\r','Information Retrieval Failed -> ',row['Org Name'],'(',row['ticker_cd'],')\\n')\n",
    "            continue\n",
    "        \n",
    "        if index > 2:\n",
    "            return corp_ppts_df , analyst_ppts_df , disclosures_df , qa_df\n",
    "        \n",
    "    return corp_ppts_df , analyst_ppts_df , disclosures_df , qa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acfab483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcripts_data_wrapper(url:str,params_iter : dict,params_bs4_filter : dict):\n",
    "    \"\"\"\n",
    "    \n",
    "    Extract data from transcripts from url \n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    url (str) : url from which data is to be parsed \n",
    "    params_iter (dict) : Dictionary of parameters that are required to iterate over the web page\n",
    "    params_bs4_filter (dict) : Dictionary of parameters that are required for effective parsing using BeatifulSoup\n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    corp_ppts_df (pandas dataframe) : Dataframe of Corporate Participants\n",
    "    analyst_ppts_df (pandas dataframe) : Dataframe of Analysts Participants\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    extracted_url_df , sel_driver = get_transcripts_urls_from_url(url , params_iter , params_bs4_filter)\n",
    "    \n",
    "    corp_ppts_df , analyst_ppts_df , disclosures_df , qa_df = get_data_from_transcripts(extracted_url_df , sel_driver)\n",
    "        \n",
    "    return extracted_url_df , corp_ppts_df , analyst_ppts_df , disclosures_df , qa_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663db51",
   "metadata": {},
   "source": [
    "## 2. Fundamentals Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ffbc349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_fundamentals_table_data(my_list:list) -> list:\n",
    "    \"\"\"\n",
    "    \n",
    "    Clean Parsed table data for fundamentals\n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    my_list (list) : Table row to be cleaned\n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    my_list (list) : Cleaned Row\n",
    "    \n",
    "    \"\"\"    \n",
    "    \n",
    "    my_list = list(map(str.strip, my_list))\n",
    "    \n",
    "    strings_to_clean = ['' ,\n",
    "                        '        ' ,\n",
    "                        '          ']\n",
    "    \n",
    "    for string in strings_to_clean:\n",
    "        try:\n",
    "            while True:\n",
    "                my_list.remove(string)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    my_list = list(map(lambda x: x.replace(\",\",\"\"), my_list))\n",
    "    \n",
    "    my_list = list(map(lambda x: x.replace(\"+\",\"\"), my_list))\n",
    "    \n",
    "    return my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "357a5e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fundamentals_data_tables(bs4_soup_data_list , params_fndmntls_data:dict , pading_cols:dict):\n",
    "    \n",
    "    data_section_tag = params_fndmntls_data.get('data_section_tag','section')\n",
    "    data_section_tag_id = params_fndmntls_data.get('data_section_tag_id','id')\n",
    "    \n",
    "    table_section_tag = params_fndmntls_data.get('table_section_tag','table')\n",
    "    table_section_subtag = params_fndmntls_data.get('table_section_subtag','class')\n",
    "\n",
    "    parsed_table_df = pd.DataFrame()\n",
    "    \n",
    "    for tables in bs4_soup_data_list.findAll(data_section_tag):\n",
    "        \n",
    "        parsed_table_inter_df = pd.DataFrame()\n",
    "        \n",
    "        for table in tables.findAll(table_section_tag):\n",
    "            \n",
    "            print('\\t',tables.get(data_section_tag_id,''),\" -> \",'Data Parsing - Started')\n",
    "            \n",
    "            rows = []\n",
    "            header = []\n",
    "            values = []\n",
    "\n",
    "            for row in table.findAll(\"tr\"):\n",
    "                values.append(row.text.split(\"\\n\"))\n",
    "            \n",
    "            values = list(map(clean_fundamentals_table_data,values))\n",
    "            \n",
    "            header = ['Metric'] + values[0]\n",
    "\n",
    "            rows = values[1:]\n",
    "\n",
    "            try:\n",
    "                parsed_table_inter_df = pd.DataFrame(rows,columns=header)\n",
    "                \n",
    "                parsed_table_inter_df = pd.melt(\n",
    "                        parsed_table_inter_df, \n",
    "                        id_vars =list(parsed_table_inter_df.columns)[0], \n",
    "                        value_vars =list(parsed_table_inter_df.columns)[1:]\n",
    "                       )\n",
    "\n",
    "                parsed_table_inter_df['fundamental_data_type'] = tables.get(data_section_tag_id,'')\n",
    "                \n",
    "                \n",
    "                if parsed_table_df.shape[0]:\n",
    "                    parsed_table_df = pd.concat(parsed_table_inter_df,parsed_table_df)\n",
    "                else:\n",
    "                    parsed_table_df = parsed_table_inter_df\n",
    "                \n",
    "                print('\\r\\t',tables.get(data_section_tag_id,''),\" -> \",'Data Parsing - Success\\n')\n",
    "                \n",
    "            except:\n",
    "                print('\\r\\t',tables.get(data_section_tag_id,''),\" -> \",'Data Parsing - Failed\\n')\n",
    "    \n",
    "    parsed_table_df[list(pading_cols.keys())] = list(pading_cols.values())\n",
    "    \n",
    "    return parsed_table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2298db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fundamentals_data_wrapper(extracted_url_df):\n",
    "    \"\"\"\n",
    "    \n",
    "    Get Funadamentals data for the identified ticker \n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    params_fundamentals (dict) : parameter for scraping fundamentals data \n",
    "    params_iter (dict) : Dictionary of parameters that are required to iterate over the web page\n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    \n",
    "    \"\"\"    \n",
    "    \n",
    "    fundamentals_data_df = pd.DataFrame()\n",
    "    \n",
    "    for index,stocks in extracted_url_df.iterrows():\n",
    "        \n",
    "        print('Fundamantals Data -> ',stocks['Org Name'],'(',stocks['ticker_cd'],')\\n')\n",
    "    \n",
    "        url = \"https://www.screener.in/company/{}/consolidated/#profit-loss\".format(stocks['ticker_cd'])\n",
    "\n",
    "        driver_data = get_driver_data(url , params_iter)\n",
    "\n",
    "        page_content_str = driver_data.page_source\n",
    "        bs4_soup_data_list = BeautifulSoup(page_content_str)\n",
    "        \n",
    "        driver_data.close()\n",
    "\n",
    "        params_fndmntls_data = {\n",
    "            'data_section_tag':'section' ,\n",
    "            'data_section_tag_id':'id' ,\n",
    "            'table_section_tag':'table' ,\n",
    "            'table_section_subtag':'class' ,\n",
    "        }\n",
    "\n",
    "        pading_cols = {\n",
    "            \"ticker_cd\" : stocks['ticker_cd'],\n",
    "            \"Org Name\" : stocks['Org Name']\n",
    "        }\n",
    "\n",
    "        inter_fundamentals_data_df = pd.DataFrame()\n",
    "        inter_fundamentals_data_df = get_fundamentals_data_tables(bs4_soup_data_list,params_fndmntls_data,pading_cols)\n",
    "        \n",
    "        if not inter_fundamentals_data_df.shape[0]:\n",
    "            pass\n",
    "        elif fundamentals_data_df.shape[0]:\n",
    "            fundamentals_data_df = pd.concat([inter_fundamentals_data_df,fundamentals_data_df])\n",
    "        else:\n",
    "            fundamentals_data_df = inter_fundamentals_data_df\n",
    "            \n",
    "    fundamentals_data_df = fundamentals_data_df.dropna().reset_index()\n",
    "    \n",
    "    fundamentals_data_df.drop(['index'],axis=1,inplace=True)\n",
    "    \n",
    "    return fundamentals_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac5c502",
   "metadata": {},
   "source": [
    "## Sample Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10687b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://alphastreet.com/india/earnings-call-transcripts/\"\n",
    "\n",
    "params_bs4_filter = {}\n",
    "params_bs4_filter['name'] = 'a'\n",
    "params_bs4_filter['href'] = True\n",
    "params_bs4_filter['attrs'] = {'rel':'bookmark'}\n",
    "params_bs4_filter['recursive'] = True\n",
    "\n",
    "params_iter = {}\n",
    "params_iter['scroll_wait_time'] = 5.0\n",
    "params_iter['iter_threshold'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64957a60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extracted_url_df , corp_ppts_df , analyst_ppts_df , disclosures_df , qa_df = get_transcripts_data_wrapper(url,params_iter,params_bs4_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da46ed6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fundamentals_data_df = get_fundamentals_data_wrapper(extracted_url_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44209393",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_url_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3746271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corp_ppts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a33b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst_ppts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49c4594",
   "metadata": {},
   "outputs": [],
   "source": [
    "disclosures_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c5937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9e4c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "fundamentals_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edbacef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c9ece6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936ae692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3e6adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_corp_ppts_list = []\n",
    "parsed_ppts_list = []\n",
    "page_content_str = None\n",
    "bs4_soup_data_list = None\n",
    "parsed_links_list = []\n",
    "\n",
    "page_content_str = driver_data.page_source\n",
    "bs4_soup_data_list = BeautifulSoup(page_content_str)\n",
    "\n",
    "\n",
    "## For Corporate PPTs\n",
    "\n",
    "params_corp_ppts = {\n",
    "    'tag_val' : 'h2',\n",
    "    'text_val' : 'Corporate Participants:'\n",
    "}\n",
    "\n",
    "target = bs4_soup_data_list.find(params_corp_ppts['tag_val'],\n",
    "                                 text=params_corp_ppts['text_val'])\n",
    "\n",
    "for sib in target.find_next_siblings():\n",
    "    if sib.name==params_corp_ppts['tag_val']:\n",
    "        break\n",
    "    else:\n",
    "        ppt_corp = transcript_header.split('(', 1)[0]\n",
    "        try:\n",
    "            ppt_name , ppt_desig = sib.text.replace(\"\\xa0\",\"\").split(\"窶能")\n",
    "        except:\n",
    "            ppt_name , ppt_desig = sib.text.split(\"\\xa0\")\n",
    "\n",
    "        parsed_corp_ppts_list.append([transcript_header , ppt_name , ppt_desig , ppt_corp])\n",
    "\n",
    "\n",
    "## For Analysts\n",
    "\n",
    "params_analyst_ppts = {\n",
    "    'tag_val' : 'h2',\n",
    "    'text_val' : 'Analysts:'\n",
    "}\n",
    "\n",
    "target = bs4_soup_data_list.find(params_analyst_ppts['tag_val'],\n",
    "                                 text=params_analyst_ppts['text_val'])\n",
    "\n",
    "\n",
    "for sib in target.find_next_siblings():\n",
    "    if sib.name==params_analyst_ppts['tag_val']:\n",
    "        break\n",
    "    else:\n",
    "        ppt_name , _ , ppt_corp_x_desig = sib.text.split(\"\\xa0\")\n",
    "\n",
    "        ppt_corp , ppt_desig =  ppt_corp_x_desig.split(\"窶能")\n",
    "\n",
    "        parsed_ppts_list.append([transcript_header , ppt_name , ppt_desig , ppt_corp])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
