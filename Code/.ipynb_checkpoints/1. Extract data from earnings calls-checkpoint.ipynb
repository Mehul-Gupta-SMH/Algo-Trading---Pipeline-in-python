{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d949987",
   "metadata": {},
   "source": [
    "# 1. Extract data from earnings calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8d14fa",
   "metadata": {},
   "source": [
    "## Adding required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7168433c",
   "metadata": {},
   "source": [
    "1. *Selenium* : https://www.selenium.dev/\n",
    "2. *Beautiful Soup* : https://beautiful-soup-4.readthedocs.io/en/latest/\n",
    "3. *WebDriver Manager* : https://www.selenium.dev/documentation/webdriver/getting_started/install_drivers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abbbee8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install selenium\n",
    "# !pip install beautifulsoup4\n",
    "# !pip3 install webdriver-manager\n",
    "# !pip3 install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39579143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2511b4",
   "metadata": {},
   "source": [
    "## Importing Selenium and Drivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ef9030",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd626bd",
   "metadata": {},
   "source": [
    "## Importing Data Processing Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f13f73aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9c7fe4",
   "metadata": {},
   "source": [
    "## 0. Common Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4c3a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_driver_data(url:str,params_iter : dict):\n",
    "    \"\"\"\n",
    "    \n",
    "    Get the html data for the page opened by Selenium driver\n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    url (str) : url from which data is to be parsed \n",
    "    params_iter (dict) : Dictionary of parameters that are required to iterate over the web page\n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    Selenium.webdriver.Chrome object : Contains data about the page that needs to be parsed \n",
    "    \n",
    "    \"\"\"    \n",
    "    \n",
    "    SCROLL_PAUSE_TIME = params_iter['scroll_wait_time']\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    \n",
    "    ## Check if the URL has correct format\n",
    "    if not url_validator(url):\n",
    "        raise ValueError(\"The URL \"&url&\" is not a valid URL format\")\n",
    "        pass\n",
    "    \n",
    "    driver.get(url)\n",
    "    \n",
    "    ## Check what was the last height of the page\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    ## Get the whole page data by loading all data from lazy loading page\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        \n",
    "        iteration += 1\n",
    "        \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "        \n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        if (new_height == last_height) or (iteration == params_iter['iter_threshold']):\n",
    "            break\n",
    "            \n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce509b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_validator(url:str) -> bool:\n",
    "    \"\"\"\n",
    "    \n",
    "    Validates if the url have correct format\n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    url (str) : url string to be checked \n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    Bool \n",
    "    \n",
    "    \"\"\"\n",
    "    regex = re.compile(\n",
    "            r'^(?:http|ftp)s?://' # http:// or https://\n",
    "            r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|' #domain...\n",
    "            r'localhost|' #localhost...\n",
    "            r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})' # ...or ip\n",
    "            r'(?::\\d+)?' # optional port\n",
    "            r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\n",
    "    \n",
    "    if re.match(regex, url):\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01fa37c",
   "metadata": {},
   "source": [
    "## 1. Parsing Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2edcab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ticker(transcript_header:str) -> str:\n",
    "    \"\"\"\n",
    "    \n",
    "    Parses out the ticker symbol from the transcript's header \n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    transcript_header (str) : header of the transcripts \n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    ticker_cd (str) : Ticker Symbol Code \n",
    "    \n",
    "    \"\"\"    \n",
    "    try:\n",
    "        ticker_cd = transcript_header[transcript_header.find(r'(')+1 : transcript_header.find(r')')]\n",
    "        return ticker_cd\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8351e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(company_name:str):\n",
    "    \"\"\"\n",
    "    \n",
    "    Get ticker symbol from Company Name\n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    company_name (str) : Company Name\n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    company_code (str) : Company code from Yahoo Finance  \n",
    "    \n",
    "    \"\"\"    \n",
    "    \n",
    "    yfinance = \"https://query2.finance.yahoo.com/v1/finance/search\"\n",
    "    user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "    params = {\"q\": company_name, \"quotes_count\": 1, \"country\": \"United States\"}\n",
    "\n",
    "    \n",
    "    res = requests.get(url=yfinance, params=params, headers={'User-Agent': user_agent})\n",
    "    data = res.json()\n",
    "\n",
    "    try:\n",
    "        meta_data = data['quotes'][0]\n",
    "\n",
    "        exchange = meta_data['exchange']\n",
    "        sector = meta_data['sector']\n",
    "        industry = meta_data['industry']\n",
    "    \n",
    "    except:\n",
    "        exchange = ''\n",
    "        sector = ''\n",
    "        industry = ''\n",
    "    \n",
    "    return [ exchange , sector , industry]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aae27ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls(driver_data,params_bs4_filter : dict) -> list:\n",
    "    \"\"\"\n",
    "    \n",
    "    Extract embedded urls from the main page in form of [header,link] pairs\n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    driver_data (Selenium.webdriver.Chrome object) : Contains data about the page that needs to be parsed \n",
    "    params_bs4_filter (dict) : Dictionary of parameters that are required for effective parsing using BeatifulSoup\n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    parsed_links_list (list) : List of [header,link] for embedded urls in the main page\n",
    "    \n",
    "    \"\"\"      \n",
    "    \n",
    "    page_content_str = None\n",
    "    bs4_soup_data_list = None\n",
    "    parsed_links_list = []\n",
    "    \n",
    "    page_content_str = driver_data.page_source\n",
    "    bs4_soup_data_list = BeautifulSoup(page_content_str)\n",
    "    \n",
    "    for links in bs4_soup_data_list.findAll(params_bs4_filter['name'],\n",
    "                                            href=params_bs4_filter['href'], \n",
    "                                            attrs=params_bs4_filter['attrs'],\n",
    "                                            recursive=params_bs4_filter['recursive']):\n",
    "        link = links['href']\n",
    "        header = links.contents[0]\n",
    "        \n",
    "        parsed_links_list.append([header,link])\n",
    "    \n",
    "    return parsed_links_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf962e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page_data_for_url(driver ,params_bs4_filter : dict):\n",
    "    \"\"\"\n",
    "    \n",
    "    Extract embedded urls from the main page in form of Pandas Dataframe\n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    url (str) : url from which data is to be parsed \n",
    "    params_iter (dict) : Dictionary of parameters that are required to iterate over the web page\n",
    "    params_bs4_filter (dict) : Dictionary of parameters that are required for effective parsing using BeatifulSoup\n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    extracted_url_df (Pandas Dataframe) : Pandas Dataframe with header and corresponding urls\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    ## Get [header,links] pairs for embedded urls \n",
    "    new_results = extract_urls(driver,params_bs4_filter)\n",
    "    \n",
    "    \n",
    "    ## Convert [header,links] pairs to pandas dataframe\n",
    "    extracted_url_df = pd.DataFrame(new_results,columns=['header','link'])\n",
    "    \n",
    "    extracted_url_df[['Org Name','temp']] = extracted_url_df['header'].str.split('(', 1,expand=True)\n",
    "    \n",
    "    extracted_url_df['ticker_cd'] = extracted_url_df['header'].map(get_ticker).to_list()\n",
    "    \n",
    "    extracted_url_df[['stock_exchange','sector','industry']] = extracted_url_df['Org Name'].map(get_metadata).to_list()\n",
    "    \n",
    "    \n",
    "    ## Data cleaning:\n",
    "    ## 1. Remove unwanted rows\n",
    "    extracted_url_df = extracted_url_df[~extracted_url_df['header'].str.contains(\"\\[\",na=True)]\n",
    "    extracted_url_df.reset_index(inplace = True)\n",
    "    extracted_url_df.drop(['index','temp'],axis=1,inplace=True)\n",
    "    \n",
    "    ## 2. Remove duplicate rows\n",
    "    extracted_url_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    return extracted_url_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c14f285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entity_participants(driver_data , transcript_header : str):\n",
    "    \"\"\"\n",
    "    \n",
    "    Extract participants from the transcripts\n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    driver_data (Selenium.webdriver.Chrome object) : Contains data about the page that needs to be parsed \n",
    "    transcript_header (str) : Company name to which the transcripts belongs to\n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    parsed_links_list (list) : List of ppts data [[],...]\n",
    "    \n",
    "    \"\"\"     \n",
    "    parsed_corp_ppts_list = []\n",
    "    parsed_ppts_list = []\n",
    "    page_content_str = None\n",
    "    bs4_soup_data_list = None\n",
    "    parsed_links_list = []\n",
    "    \n",
    "    page_content_str = driver_data.page_source\n",
    "    bs4_soup_data_list = BeautifulSoup(page_content_str)\n",
    "    \n",
    "    \n",
    "    ## For Corporate PPTs\n",
    "    \n",
    "    params_corp_ppts = {\n",
    "        'tag_val' : 'h2',\n",
    "        'text_val' : 'Corporate Participants:'\n",
    "    }\n",
    "    \n",
    "    target = bs4_soup_data_list.find(params_corp_ppts['tag_val'],\n",
    "                                     text=params_corp_ppts['text_val'])\n",
    "\n",
    "    for sib in target.find_next_siblings():\n",
    "        if sib.name==params_corp_ppts['tag_val']:\n",
    "            break\n",
    "        else:\n",
    "            ppt_corp = transcript_header.split('(', 1)[0]\n",
    "            try:\n",
    "                ppt_name , ppt_desig = sib.text.split(\"\\xa0—\\xa0\")\n",
    "            except:\n",
    "                ppt_name , ppt_desig = sib.text.split(\"\\xa0\")\n",
    "\n",
    "            parsed_corp_ppts_list.append([transcript_header , ppt_name , ppt_desig , ppt_corp])\n",
    "    \n",
    "          \n",
    "    ## For Analysts\n",
    "    \n",
    "    params_analyst_ppts = {\n",
    "        'tag_val' : 'h2',\n",
    "        'text_val' : 'Analysts:'\n",
    "    }\n",
    "    \n",
    "    target = bs4_soup_data_list.find(params_analyst_ppts['tag_val'],\n",
    "                                     text=params_analyst_ppts['text_val'])\n",
    "    \n",
    "\n",
    "    for sib in target.find_next_siblings():\n",
    "        if sib.name==params_analyst_ppts['tag_val']:\n",
    "            break\n",
    "        else:\n",
    "            ppt_name , _ , ppt_corp_x_desig = sib.text.split(\"\\xa0\")\n",
    "            \n",
    "            ppt_corp , ppt_desig =  ppt_corp_x_desig.split(\"—\")\n",
    "\n",
    "            parsed_ppts_list.append([transcript_header , ppt_name , ppt_desig , ppt_corp])\n",
    "            \n",
    "    \n",
    "    \n",
    "    return parsed_corp_ppts_list,parsed_ppts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a744861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcripts_urls_from_url(url:str,params_iter : dict,params_bs4_filter : dict):\n",
    "    \"\"\"\n",
    "    \n",
    "    Extract url of transcripts from main url\n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    url (str) : url from which data is to be parsed \n",
    "    params_iter (dict) : Dictionary of parameters that are required to iterate over the web page\n",
    "    params_bs4_filter (dict) : Dictionary of parameters that are required for effective parsing using BeatifulSoup\n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    extracted_url_df (pandas dataframe) : Dataframe of extrated urls\n",
    "    \n",
    "    \"\"\"   \n",
    "    \n",
    "    sel_driver = get_driver_data(url , params_iter)\n",
    "    \n",
    "    extracted_url_df = parse_page_data_for_url(sel_driver , params_bs4_filter)\n",
    "    \n",
    "    return extracted_url_df , sel_driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a0af2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_disclosures_from_transcripts(driver_data,pading_cols:dict):\n",
    "    \n",
    "    page_content_str = driver_data.page_source\n",
    "    bs4_soup_data_list = BeautifulSoup(page_content_str)\n",
    "    \n",
    "    header = None\n",
    "\n",
    "    value_list = []\n",
    "\n",
    "    for values in bs4_soup_data_list.find('h2',text='Presentation:').find_next_siblings():\n",
    "\n",
    "        if values.find('span') is not None:\n",
    "            continue\n",
    "\n",
    "        if values.find('strong') is not None:\n",
    "            header = values.text\n",
    "\n",
    "        else:\n",
    "            value_list.append([header,values.text])\n",
    "\n",
    "    data_df = pd.DataFrame(value_list,columns=['said_by','info'])\n",
    "    \n",
    "    data_df[list(pading_cols.keys())] = list(pading_cols.values())\n",
    "    \n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5989848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_answers_from_transcripts(driver_data,pading_cols:dict):\n",
    "    \n",
    "    page_content_str = driver_data.page_source\n",
    "    bs4_soup_data_list = BeautifulSoup(page_content_str)    \n",
    "\n",
    "    header = None\n",
    "\n",
    "    question = []\n",
    "\n",
    "    value_list = []\n",
    "\n",
    "    for values in bs4_soup_data_list.find('h2',text='Questions and Answers:').find_next_siblings():\n",
    "\n",
    "        if values.find('span') is not None:\n",
    "            continue\n",
    "\n",
    "        if values.find('strong') is not None:\n",
    "            header = values.text\n",
    "\n",
    "        else:\n",
    "            if header == 'Operator':\n",
    "                continue\n",
    "            if 'Analyst' in header:\n",
    "                question = [header,values.text]\n",
    "            else:\n",
    "                value_list.append([question[0],question[1],header,values.text])\n",
    "\n",
    "    data_df = pd.DataFrame(value_list,columns=['question_by','question','answer_by','answer'])\n",
    "    \n",
    "    data_df[list(pading_cols.keys())] = list(pading_cols.values())\n",
    "    \n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a12684ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_transcripts(extracted_url_df : pd.DataFrame , sel_driver):\n",
    "    \"\"\"\n",
    "    \n",
    "    Extract data from transcripts from url dataframe\n",
    "    1. Participants Data\n",
    "        a. Corporate Participants\n",
    "        b. Analysts Participants\n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    url (str) : url from which data is to be parsed \n",
    "    params_iter (dict) : Dictionary of parameters that are required to iterate over the web page\n",
    "    params_bs4_filter (dict) : Dictionary of parameters that are required for effective parsing using BeatifulSoup\n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    corp_ppts_df (pandas dataframe) : Dataframe of Corporate Participants\n",
    "    analyst_ppts_df (pandas dataframe) : Dataframe of Analysts Participants\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    corp_ppts_df = pd.DataFrame()\n",
    "    analyst_ppts_df = pd.DataFrame()\n",
    "    disclosures_df = pd.DataFrame()\n",
    "    qa_df = pd.DataFrame()\n",
    "    \n",
    "    for index,row in extracted_url_df.iterrows():\n",
    "        \n",
    "        try:\n",
    "            sel_driver.get(row['link'])\n",
    "            \n",
    "            pading_cols = {\n",
    "                \"ticker_cd\" : row['ticker_cd'],\n",
    "                \"Org Name\" : row['Org Name']\n",
    "            }\n",
    "\n",
    "            \n",
    "            inter_disclosures_df = get_disclosures_from_transcripts(sel_driver,pading_cols)\n",
    "            \n",
    "            inter_qa_df = get_question_answers_from_transcripts(sel_driver,pading_cols)\n",
    "        \n",
    "            corp_ppts_list , analyst_ppts_list = extract_entity_participants(sel_driver,row['header'])\n",
    "            \n",
    "            inter_corp_ppts_df = pd.DataFrame(corp_ppts_list,columns=['Transcript Header','Name','Designation','Corp Name'])\n",
    "\n",
    "            inter_analyst_ppts_df = pd.DataFrame(analyst_ppts_list,columns=['Transcript Header','Name','Designation','Corp Name'])\n",
    "            \n",
    "            if corp_ppts_df.shape[1] > 0:\n",
    "                corp_ppts_df = pd.concat(corp_ppts_df,inter_corp_ppts_df)\n",
    "                analyst_ppts_df = pd.concat(analyst_ppts_df,inter_analyst_ppts_df)\n",
    "                disclosures_df = pd.concat(disclosures_df,inter_disclosures_df)\n",
    "                qa_df = pd.concat(qa_df,inter_qa_df)\n",
    "            else:\n",
    "                corp_ppts_df = inter_corp_ppts_df\n",
    "                analyst_ppts_df = inter_analyst_ppts_df\n",
    "                disclosures_df = inter_disclosures_df\n",
    "                qa_df = inter_qa_df\n",
    "                \n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        if index > 2:\n",
    "            return corp_ppts_df , analyst_ppts_df , disclosures_df , qa_df\n",
    "        \n",
    "    return corp_ppts_df , analyst_ppts_df , disclosures_df , qa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acfab483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcripts_data_wrapper(url:str,params_iter : dict,params_bs4_filter : dict):\n",
    "    \"\"\"\n",
    "    \n",
    "    Extract data from transcripts from url \n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    url (str) : url from which data is to be parsed \n",
    "    params_iter (dict) : Dictionary of parameters that are required to iterate over the web page\n",
    "    params_bs4_filter (dict) : Dictionary of parameters that are required for effective parsing using BeatifulSoup\n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    corp_ppts_df (pandas dataframe) : Dataframe of Corporate Participants\n",
    "    analyst_ppts_df (pandas dataframe) : Dataframe of Analysts Participants\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    extracted_url_df , sel_driver = get_transcripts_urls_from_url(url , params_iter , params_bs4_filter)\n",
    "    \n",
    "    corp_ppts_df , analyst_ppts_df , disclosures_df , qa_df = get_data_from_transcripts(extracted_url_df , sel_driver)\n",
    "        \n",
    "    return extracted_url_df , corp_ppts_df , analyst_ppts_df , disclosures_df , qa_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663db51",
   "metadata": {},
   "source": [
    "## 2. Fundamentals Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ffbc349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_fundamentals_table_data(my_list:list) -> list:\n",
    "    \"\"\"\n",
    "    \n",
    "    Clean Parsed table data for fundamentals\n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    my_list (list) : Table row to be cleaned\n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    my_list (list) : Cleaned Row\n",
    "    \n",
    "    \"\"\"    \n",
    "    \n",
    "    my_list = list(map(str.strip, my_list))\n",
    "    \n",
    "    strings_to_clean = ['' ,\n",
    "                        '        ' ,\n",
    "                        '          ']\n",
    "    \n",
    "    for string in strings_to_clean:\n",
    "        try:\n",
    "            while True:\n",
    "                my_list.remove(string)\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    my_list = list(map(lambda x: x.replace(\",\",\"\"), my_list))\n",
    "    \n",
    "    my_list = list(map(lambda x: x.replace(\"+\",\"\"), my_list))\n",
    "    \n",
    "    return my_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49d1dc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fundamentals_data_tables(bs4_soup_data_list , params_fndmntls_data:dict , pading_cols:dict):\n",
    "    \n",
    "    data_section_tag = params_fndmntls_data.get('data_section_tag','section')\n",
    "    data_section_tag_id = params_fndmntls_data.get('data_section_tag_id','id')\n",
    "    \n",
    "    table_section_tag = params_fndmntls_data.get('table_section_tag','table')\n",
    "    table_section_subtag = params_fndmntls_data.get('table_section_subtag','class')\n",
    "\n",
    "    parsed_table_df = pd.DataFrame()\n",
    "    \n",
    "    for tables in bs4_soup_data_list.findAll(data_section_tag):\n",
    "        \n",
    "        parsed_table_inter_df = pd.DataFrame()\n",
    "        \n",
    "        for table in tables.findAll(table_section_tag):\n",
    "            \n",
    "            rows = []\n",
    "            header = []\n",
    "            values = []\n",
    "\n",
    "            for row in table.findAll(\"tr\"):\n",
    "                values.append(row.text.split(\"\\n\"))\n",
    "            \n",
    "            values = list(map(clean_fundamentals_table_data,values))\n",
    "            \n",
    "            header = ['Metric'] + values[0]\n",
    "\n",
    "            rows = values[1:]\n",
    "\n",
    "            try:\n",
    "                parsed_table_inter_df = pd.DataFrame(rows,columns=header)\n",
    "                \n",
    "                parsed_table_inter_df = pd.melt(\n",
    "                        parsed_table_inter_df, \n",
    "                        id_vars =list(parsed_table_inter_df.columns)[0], \n",
    "                        value_vars =list(parsed_table_inter_df.columns)[1:]\n",
    "                       )\n",
    "\n",
    "                parsed_table_inter_df['fundamental_data_type'] = tables.get(data_section_tag_id,'')\n",
    "                \n",
    "                \n",
    "                if parsed_table_df.shape[0]:\n",
    "                    parsed_table_df = pd.concat(parsed_table_inter_df,parsed_table_df)\n",
    "                else:\n",
    "                    parsed_table_df = parsed_table_inter_df\n",
    "\n",
    "            except:\n",
    "                print(tables.get(data_section_tag_id,''),\" -> \",'Data Unparsed')\n",
    "    \n",
    "    parsed_table_df[list(pading_cols.keys())] = list(pading_cols.values())\n",
    "    \n",
    "    return parsed_table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2298db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fundamentals_data_wrapper(extracted_url_df):\n",
    "    \"\"\"\n",
    "    \n",
    "    Get Funadamentals data for the identified ticker \n",
    "    ------------------------------------\n",
    "    \n",
    "    Input:\n",
    "    params_fundamentals (dict) : parameter for scraping fundamentals data \n",
    "    params_iter (dict) : Dictionary of parameters that are required to iterate over the web page\n",
    "    \n",
    "    ------------------------------------\n",
    "    Output:\n",
    "    \n",
    "    \"\"\"    \n",
    "    \n",
    "    fundamentals_data_df = pd.DataFrame()\n",
    "    \n",
    "    for index,stocks in extracted_url_df.iterrows():\n",
    "    \n",
    "        url = \"https://www.screener.in/company/{}/consolidated/#profit-loss\".format(stocks['ticker_cd'])\n",
    "\n",
    "        driver_data = get_driver_data(url , params_iter)\n",
    "\n",
    "        page_content_str = driver_data.page_source\n",
    "        bs4_soup_data_list = BeautifulSoup(page_content_str)\n",
    "        \n",
    "        driver_data.close()\n",
    "\n",
    "        params_fndmntls_data = {\n",
    "            'data_section_tag':'section' ,\n",
    "            'data_section_tag_id':'id' ,\n",
    "            'table_section_tag':'table' ,\n",
    "            'table_section_subtag':'class' ,\n",
    "        }\n",
    "\n",
    "        pading_cols = {\n",
    "            \"ticker_cd\" : stocks['ticker_cd'],\n",
    "            \"Org Name\" : stocks['Org Name']\n",
    "        }\n",
    "\n",
    "        inter_fundamentals_data_df = pd.DataFrame()\n",
    "        inter_fundamentals_data_df = get_fundamentals_data_tables(bs4_soup_data_list,params_fndmntls_data,pading_cols)\n",
    "        \n",
    "        if not inter_fundamentals_data_df.shape[0]:\n",
    "            pass\n",
    "        elif fundamentals_data_df.shape[0]:\n",
    "            fundamentals_data_df = pd.concat(fundamentals_data_df,inter_fundamentals_data_df)\n",
    "        else:\n",
    "            fundamentals_data_df = inter_fundamentals_data_df\n",
    "            \n",
    "    fundamentals_data_df = fundamentals_data_df.dropna().reset_index()\n",
    "    \n",
    "    fundamentals_data_df.drop(['index'],axis=1,inplace=True)\n",
    "    \n",
    "    return fundamentals_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac5c502",
   "metadata": {},
   "source": [
    "## Sample Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b10687b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://alphastreet.com/india/earnings-call-transcripts/\"\n",
    "\n",
    "params_bs4_filter = {}\n",
    "params_bs4_filter['name'] = 'a'\n",
    "params_bs4_filter['href'] = True\n",
    "params_bs4_filter['attrs'] = {'rel':'bookmark'}\n",
    "params_bs4_filter['recursive'] = True\n",
    "\n",
    "params_iter = {}\n",
    "params_iter['scroll_wait_time'] = 5.0\n",
    "params_iter['iter_threshold'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64957a60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extracted_url_df , corp_ppts_df , analyst_ppts_df , disclosures_df , qa_df = get_transcripts_data_wrapper(url,params_iter,params_bs4_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da46ed6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fundamentals_data_df = get_fundamentals_data_wrapper(extracted_url_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44209393",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_url_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3746271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corp_ppts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a33b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst_ppts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b28d818",
   "metadata": {},
   "outputs": [],
   "source": [
    "disclosures_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4445960",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "164ffba5",
   "metadata": {},
   "source": [
    "a, b = get_transcripts_urls_from_url(url,params_iter,params_bs4_filter)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b20c590d",
   "metadata": {},
   "source": [
    "driver_data = b\n",
    "page_content_str = driver_data.page_source\n",
    "bs4_soup_data_list = BeautifulSoup(page_content_str)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c9d4137",
   "metadata": {},
   "source": [
    "driver_data.get('https://alphastreet.com/india/metropolis-healthcare-limited-metropolis-q4-fy23-earnings-concall-transcript/')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3779b6dc",
   "metadata": {},
   "source": [
    "page_content_str = driver_data.page_source\n",
    "bs4_soup_data_list = BeautifulSoup(page_content_str)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c92505e",
   "metadata": {},
   "source": [
    "bs4_soup_data_list"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fad3d4b5",
   "metadata": {},
   "source": [
    "for values in bs4_soup_data_list.find('h2',text='Presentation:').find_next_siblings():\n",
    "#     for value in values.findAll('strong'):\n",
    "#         print(value)\n",
    "#         for subvalue in value.find_next_siblings():\n",
    "#             print(subvalue)\n",
    "#         print(\"\\n\\n************\\n\\n\")\n",
    "#     print(values)\n",
    "    print(\"\\n\\n======================\\n\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "730031ef",
   "metadata": {},
   "source": [
    "def get_disclosures_from_transcripts(bs4_soup_data_list,pading_cols:dict):\n",
    "    header = None\n",
    "\n",
    "    value_list = []\n",
    "\n",
    "    for values in bs4_soup_data_list.find('h2',text='Presentation:').find_next_siblings():\n",
    "\n",
    "        if values.find('span') is not None:\n",
    "            continue\n",
    "\n",
    "        if values.find('strong') is not None:\n",
    "            header = values.text\n",
    "\n",
    "        else:\n",
    "            value_list.append([header,values.text])\n",
    "\n",
    "    data_df = pd.DataFrame(value_list,columns=['said_by','info'])\n",
    "    \n",
    "    data_df[list(pading_cols.keys())] = list(pading_cols.values())\n",
    "    \n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fc0956e7",
   "metadata": {},
   "source": [
    "def get_question_answers_from_transcripts(bs4_soup_data_list,pading_cols:dict):\n",
    "\n",
    "    header = None\n",
    "\n",
    "    question = []\n",
    "\n",
    "    value_list = []\n",
    "\n",
    "    for values in bs4_soup_data_list.find('h2',text='Questions and Answers:').find_next_siblings():\n",
    "\n",
    "        if values.find('span') is not None:\n",
    "            continue\n",
    "\n",
    "        if values.find('strong') is not None:\n",
    "            header = values.text\n",
    "\n",
    "        else:\n",
    "            if header == 'Operator':\n",
    "                continue\n",
    "            if 'Analyst' in header:\n",
    "                question = [header,values.text]\n",
    "            else:\n",
    "                value_list.append([question[0],question[1],header,values.text])\n",
    "\n",
    "    data_df = pd.DataFrame(value_list,columns=['question_by','question','answer_by','answer'])\n",
    "    \n",
    "    data_df[list(pading_cols.keys())] = list(pading_cols.values())\n",
    "    \n",
    "    return data_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
